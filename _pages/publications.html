---
layout: archive
title: "Publications"
permalink: /publications/
author_profile: true
---
{% if site.author.googlescholar %}
  <div class="wordwrap">You can also find my articles on <a href="{{site.author.googlescholar}}">my Google Scholar profile</a>.</div>
{% endif %}
{% include base_path %}

<h2>Publications</h2>
<ul>
  <li>
    <strong>Economics of ChatGPT: A Labor Market View on the Occupational Impact of Artificial Intelligence</strong> <a href="https://www.emerald.com/insight/content/doi/10.1108/JEBDE-10-2023-0021/full/html">[Link]</a>
    <details>
      <summary>Abstract</summary>
      <p>This paper examines the potential impact of ChatGPT and similar large language models on labor markets. We analyze occupational vulnerability using task content and skill requirements, finding that high-skilled occupations may face significant disruption while identifying opportunities for human-AI collaboration and workforce adaptation.</p>
    </details>
  </li>
</ul>

<h2>Working Papers</h2>
<ul>
  <li>
    <strong>Evidence on Inflation Expectations Formation Using Large Language Models</strong> <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4825076">[Draft]</a>
    <details>
      <summary>Abstract</summary>
      <p>This study investigates how large language models form inflation expectations compared to human forecasters. Using experimental methods, we demonstrate that LLMs exhibit similar patterns of expectation formation, including anchoring biases and information processing limitations, with implications for macroeconomic modeling and AI augmentation of forecasting processes.</p>
    </details>
    <p><strong>Presentations</strong>: ICD Departmental Seminar Series (IMF), 2024 North American Meeting of the Economic Science Association (Columbus, OH), Experimental Economics Workshop at Purdue University (West Lafayette, IN), 30th Midwest Macroeconomics Meetings (West Lafayette, IN), 5th ACM International Conference on AI in Finance (Brooklyn, NY)</p>
    <p>Coverage: <a href="https://marginalrevolution.com/marginalrevolution/2024/05/experimental-evidence-on-large-language-models.html">Marginal Revolution</a></p>
  </li>
  
  <li>
    <strong>Simulating the Survey of Professional Forecasters</strong> <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5066286">[Draft]</a>
    <details>
      <summary>Abstract</summary>
      <p>This paper evaluates large language models' ability to simulate professional economic forecasters. We compare LLM-generated forecasts to the Survey of Professional Forecasters, finding that models can replicate key statistical properties of human forecasts including accuracy, dispersion, and anchoring effects, with implications for macroeconomic research and simulation.</p>
    </details>
    <p>with Anne Lundgaard Hansen (Federal Reserve Bank of Richmond), John J. Horton (MIT Sloan), Sophia Kazinnik (Stanford HAI), & Daniela Puzzello (Indiana University)</p>
    <p>Coverage: <a href="https://www.federalreserve.gov/newsevents/speech/barr20250218a.htm">Vice Chair for Supervision Michael S. Barr Speech</a> â€¢ <a href="https://www.bloomberg.com/opinion/articles/2025-02-26/robots-will-write-the-macro-forecasts">Bloomberg</a></p>
  </li>
  
  <li>
    <strong>Understanding AI Agents' Decision-Making: Evidence from Risk and Time Preference Elicitation</strong> <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5154002">[Draft]</a>
    <details>
      <summary>Abstract</summary>
      <p>This research explores decision-making behaviors of large language models through experimental economics methods. By eliciting risk and time preferences using standard economic tasks, we characterize AI agents' behavioral patterns, compare them to human benchmarks, and analyze implications for human-AI decision alignment.</p>
    </details>
    <p>with Gajanan L Ganji (University of Texas at Arlington)</p>
  </li>
</ul>

<h2>Works in Progress</h2>
<ul>
  <li>
    <strong>The AI Persuasion Project</strong> <a href="https://sites.google.com/view/ai-persuasion/team?authuser=0">[Website]</a>
    <details>
      <summary>Abstract</summary>
      <p>A cross-disciplinary big-team science effort to study LLM capability evaluation in the context of persuasion. This project brings together researchers from economics, computer science, and psychology to develop standardized methods for assessing AI systems' ability to influence human decision-making across diverse contexts.</p>
    </details>
  </li>
</ul>

<!-- New style rendering if publication categories are defined -->
{% if site.publication_category %}
  {% for category in site.publication_category  %}
    {% assign title_shown = false %}
    {% for post in site.publications reversed %}
      {% if post.category != category[0] %}
        {% continue %}
      {% endif %}
      {% unless title_shown %}
        <h2>{{ category[1].title }}</h2><hr />
        {% assign title_shown = true %}
      {% endunless %}
      {% include archive-single.html %}
    {% endfor %}
  {% endfor %}
{% else %}
  {% for post in site.publications reversed %}
    {% include archive-single.html %}
  {% endfor %}
{% endif %}
